# CSET419-Gen-AI-E23CSEU1603

# LAB-1
# Objective
The objective of this experiment is to understand the fundamentals of Generative AI by generating synthetic medical imaging data using a pre-trained generative model. Students will learn how to use Python-based tools to generate, store, and visualize synthetic X-ray samples.

# Experiment 1: Implement a Simple Generative Algorithm for Medical Data Generation
Generative AI models are capable of creating new and realistic data samples based on learned patterns. In this experiment, a pre-trained generative model is used to generate synthetic X-ray images, which are then stored in a structured dataset format for further analysis, such as training diagnostic models.

# Domain Selected
Medical Image Generation (Chest/Bone X-Rays)

# Generative Model Used
Stable Diffusion (Pre-trained diffusion-based generative model adapted for medical imaging prompts)

# Methodology / Procedure
Selected Medical X-ray Imaging as the data domain.
Loaded a pre-trained Stable Diffusion model using Python.
Provided multiple medical textual input prompts (minimum of 5) to guide the generation process (e.g., "Frontal chest X-ray of healthy lungs", "X-ray of a fractured radius bone").
Generated synthetic X-ray images using the generative model.
Saved the generated images in a folder-based dataset structure.
Displayed sample generated outputs for visual verification of anatomical structure.

# Tools & Technologies
Python
Google Colab
Stable Diffusion
PyTorch
Hugging Face Diffusers Library

# Output
A synthetic X-ray dataset generated using a pre-trained generative AI model.
Images stored in an organized folder structure.
Sample outputs displayed to verify data quality and anatomical realism.

# Conclusion
This experiment demonstrates how Generative AI models can be used to efficiently create synthetic medical datasets. The generated data can be further utilized for tasks such as training diagnostic machine learning models, data augmentation for rare conditions, and experimentation, especially in medical scenarios where real-world patient data is privacy-sensitive or limited.



# LAB-2
Summary
This week's project focuses on building and training a Generative Adversarial Network (GAN) to generate synthetic images of handwritten digits that resemble the MNIST dataset. The quality of these generated images is then evaluated using a pre-trained LeNet-5 classifier.

# Key Features:
Dynamic GAN Configuration: The GAN's hyperparameters, such as the number of epochs, batch size, and noise dimension, can be dynamically configured by the user, allowing for experimentation and fine-tuning.

Data Loading and Preprocessing: The MNIST dataset is loaded and preprocessed, with images normalized to a range of [-1, 1] to match the generator's output activation function (tanh).

# Generator and Discriminator Architecture:

The Generator uses Conv2DTranspose layers to upsample a random noise vector into a 28x28 grayscale image.
The Discriminator is a standard Convolutional Neural Network (CNN) designed to classify images as either real (from the MNIST dataset) or fake (generated by the generator).
GAN Training: The GAN is trained in a loop where the generator and discriminator are pitted against each other. The generator learns to produce more realistic images, while the discriminator gets better at distinguishing real from fake.

# Image Generation and Classification:
After training, the generator produces a set of synthetic digit images. These images are then classified by a pre-trained LeNet-5 model, and the results are used to sort the images into folders corresponding to their predicted labels (0-9).

# Performance Evaluation:
The accuracy of the LeNet-5 classifier on the GAN-generated images is calculated to assess the quality and realism of the synthetic digits. A detailed breakdown of predictions is also provided to offer insights into the classifier's performance on individual images.


# LAB-3

Summary
# Objective
The objective of this experiment is to implement and analyze a Variational Autoencoder (VAE). The goal is to design Encoder-Decoder architectures that learn a probabilistic latent space, allowing for both the efficient reconstruction of input data and the generation of diverse new synthetic samples by sampling from a learned distribution.

# Experiment Overview
In this experiment, we explore the capabilities of VAEs in learning continuous, structured representations of data. Unlike standard autoencoders which simply memorize inputs, VAEs learn a "smooth" latent space. The task involves training a model on fashion items to understand distinct features (e.g., the structural difference between a boot and a sneaker) and utilizing this understanding to generate novel clothing designs from random noise.

# Domain & Model
 .Domain Selected: Fashion-MNIST
    .A dataset of 28x28 grayscale images representing 10 categories of clothing and accessories.
.Generative Model: Variational Autoencoder (VAE)
     . Probabilistic Encoder: Maps inputs to a distribution (defined by mean μ and variance σ ).
     . Decoder: Reconstructs images from sampled latent vectors.
# Methodology
1 .Dataset Preparation: Loaded and preprocessed the Fashion-MNIST dataset.

2 Architecture Design:

 . Implemented an Encoder to compress input images into a lower-dimensional latent distribution.
 
 . Applied the Reparameterization Trick to enable backpropagation through stochastic sampling.
 
 . Implemented a Decoder to reconstruct the original image from the sampled latent vectors.
 
3 Training Process: Trained the model by minimizing a dual loss function:

 . Reconstruction Loss: Ensures the output visually resembles the input.
 
 . KL Divergence: Regularizes the learned distribution to approximate a standard normal distribution.
 
4 Analysis & Visualization:

 . Reconstruction Analysis: Plotting original inputs against reconstructed versions to check data compression quality.
 
 . Generative Sampling: generating completely new images by sampling random noise vectors ($z \sim \mathcal{N}(0,1)$).
 
 . Latent Space Visualization: Projecting the test dataset into a 2D latent space to visualize clustering of class labels.
 
# Tools & Technologies

. Language: Python

. Platform: Google Colab

. Framework: PyTorch

. Visualization: Matplotlib

# Key Outputs

1.Reconstruction Grid: A visual comparison of original Fashion-MNIST images (Top row) vs. VAE-reconstructed counterparts (Bottom row) to verify feature retention.

2.Generative Samples: A grid of distinct, newly generated fashion items created solely from random noise

3.Latent Space Scatter Plot: A 2D scatter plot where data points are colored by class label, demonstrating how the model groups semantically similar items (e.g., T-shirts vs. Trousers) in the latent space.

# Conclusion

This experiment validates the effectiveness of Variational Autoencoders in learning structured data representations. The reconstruction results demonstrate the model's ability to capture essential features, while the generative samples prove it can synthesize novel data points. Furthermore, the latent space visualization confirms that the VAE successfully clusters semantically similar items, proving its utility for unsupervised learning and creative generation tasks.

# LAB-4
. N-Gram Model: Implemented statistical n-gram based text generation using bigram sequences and random sampling.

. LSTM Text Generation: Built an RNN/LSTM architecture with embedding layer to generate contextually relevant text sequences.

. Transformer Architecture: Implemented a custom Transformer model with multi-head self-attention, positional embeddings, and feed-forward networks.

. Custom Corpus: Trained models on a curated dataset of 40+ sentences covering AI, machine learning, and NLP topics.

. Model Comparison: Analyzed trade-offs between statistical models, sequential neural networks, and attention-based transformers.

. Text Generation: Generated meaningful text using seed words with all three model architectures.

. Attention Mechanisms: Explored parallel processing and contextual understanding using transformer self-attention.
